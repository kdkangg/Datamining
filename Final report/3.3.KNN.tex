\section{KNN}
The k-nearest neighbor (kNN) method is one of the more popular classification methods in data mining and statistics, and its most notable features are its simplicity of implementation and remarkable classification performance[F1].
The model-free k-nearest neighbours (kNNs) method performs the classification task in two main steps. Firstly the calculation of the distance between the test samples and all the training samples is performed and ranked by ascending order. Secondly the labels of the selected nearest neighbours are assigned to the test samples according to the majority rule, cum kNN classification[F1].The common kNN method consists of two types, one specifies a fixed optimal k value (expert preset) to be applied to all test samples[F1][F2][F3][F4];The other specifies a different optimal k values to be applied to different test samples[F5][F6]. For instance, Sharma and Lall[F7] state that the fixed optimal k value for all test samples should be k =√n(where n > 100 and n is the number of training samples), while Zhu et al[F6] proposed to select different optimal k values for test samples by the tenfold cross-validation method. However, the traditional kNN method, which assigns a fixed value of K to all test samples, has proven to be less applicable in practice[F1]. Besides for classification tasks, the kNN method is further used for regression and imputation of missing data[F8]. kNN has two relatively significant drawbacks, the first being its relatively low operational efficiency – as a lazy learning method, it is banned in many applications, such as dynamic web mining for large repositories and secondly the choice of k value has an important impact on the results[F9].
In the preprocessing our group used the kNN imputation method designed according to Minkowski distance or its variants to handle missing data appropriately and to improve results of data analysis, generally this is a valid approach for numeric variables (features or attributes)[F10]. Under the Normalized Root Mean Square Error (RMSE) method kNN has shown the superiority over some other methods, for example, mean imputation, median imputation, predictive mean matching, Bayesian Linear Regression (norm), Linear Regression, non-Bayesian (norm.nob), and random sample[F11]. Our group chose to use the mean value per person to fill in the missing data. Then normalised the data. Generally speaking any type of problem is normalised in the pre-processing phase, especially in areas such as soft computing and cloud computing. The range of data is scaled according to the needs of the problem[F12].
kNN parameter selection was carried out using GridSearchCV. The parameters studied were the number of nearest neighbours, the metric of distance, weighting functions, and the leaf_size. The most popular parameter choices were evaluated, including nine k-values, three popular distance measures and two well-known weighting functions[F13].

\section{Reference}
[F1] Zhang, S., Li, X., Zong, M., Zhu, X. and Wang, R., 2017. Efficient kNN classification with different numbers of nearest neighbors. IEEE transactions on neural networks and learning systems, 29(5), pp.1774-1785.
[F2] Zhang, S., 2008. Parimputation: From imputation and null-imputation to partially imputation. IEEE Intell. Informatics Bull., 9(1), pp.32-38. 
[F3] Góra, G. and Wojna, A., 2002, August. RIONA: A classifier combining rule induction and k-NN method with automated selection of optimal neighbourhood. In European Conference on Machine Learning (pp. 111-123).
[F4] Springer, Berlin, Heidelberg. Li, B., Chen, Y.W. and Chen, Y.Q., 2008. The nearest neighbor algorithm of local probability centers. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 38(1), pp.141-154.
[F5] Wang, J., Neskovic, P. and Cooper, L.N., 2006. Neighborhood size selection in the k-nearest-neighbor rule using statistical confidence. Pattern Recognition, 39(3), pp.417-423. 
[F6] Zhu, X., Zhang, S., Jin, Z., Zhang, Z. and Xu, Z., 2010. Missing value estimation for mixed-attribute data sets. IEEE Transactions on Knowledge and Data Engineering, 23(1), pp.110-121.
[F7] Lall, U. and Sharma, A., 1996. A nearest neighbor bootstrap for resampling hydrologic time series. Water resources research, 32(3), pp.679-693.
[F8] Zhang, S., Li, X., Zong, M., Zhu, X. and Cheng, D., 2017. Learning k for knn classification. ACM Transactions on Intelligent Systems and Technology (TIST), 8(3), pp.1-19.
[F9] Guo, G., Wang, H., Bell, D., Bi, Y. and Greer, K., 2003, November. KNN model-based approach in classification. In OTM Confederated International Conferences" On the Move to Meaningful Internet Systems" (pp. 986-996). Springer, Berlin, Heidelberg.
[F10] Zhang, S., 2012. Nearest neighbor selection for iteratively kNN imputation. Journal of Systems and Software, 85(11), pp.2541-2552.
[F11] Jadhav, A., Pramod, D. and Ramanathan, K., 2019. Comparison of performance of data imputation methods for numeric dataset. Applied Artificial Intelligence, 33(10), pp.913-933.
[F12] Patro, S. and Sahu, K.K., 2015. Normalization: A preprocessing stage. arXiv preprint arXiv:1503.06462.
[F13] Batista, G.E.A.P.A. and Silva, D.F., 2009, August. How k-nearest neighbor parameters affect its performance. In Argentine symposium on artificial intelligence (pp. 1-12).
