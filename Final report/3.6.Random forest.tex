\section{Random forest}
Due to the unique advantages of RF in handling small sample sizes, high-dimensional feature spaces and complex data structures [1], random Forest is a very common algorithm in machine learning and one of the most practical algorithms for bagging integration strategies. A random forest (RF) classifier uses a randomly selected subset of training samples and variables to generate multiple decision trees [2] and its natural incorporation of feature selection and interactions in the learning process [1]. A random forest is an ensemble of unpruned classification or regression trees created by using bootstrap samples of training data and random feature selection of the trees. Predictions are made by aggregating (majority voting or averaging) the predictions of the ensemble [3]. To meet the diversity requirement, random sampling of the dataset is required, which includes random sampling of samples and random sampling of features, with the aim of giving each tree a personality. It has been noted that random forests may have biased feature selection for individual trees [4]. Therefore, if secondary features are selected, the classification accuracy will be affected [5].
In order to obtain valid results, the parameters of the random forest had to be carefully tuned [6]. We chose GridSearchCV for parameter optimisation and tuning. The number of trees in a supervised learning Random Forest (RF) algorithm is set by the user [7]. More trees mean more overall capability, but modelling too many trees will result in some reduction in overall efficiency, and there is also a time cost to consider, so it is not the case that the greater the number of trees, the superior the performance of the forest. Doubling the number of trees is of no value, and there is no significant benefit beyond a threshold unless there is a huge computing environment [8]. So, we selected five representative values for the tree count screening. A variety of partitioning criteria exist in random forests. For example, Breiman et al [9] proposed the classical tree CART based on Gini impurity, which measures the probability that a randomly selected sample in the sample set is split wrongly. a smaller Gini index means that the probability that a selected sample in the set is split wrongly is smaller, which means that the set is more pure, and vice versa, the set is less pure. Quinlan [10] proposed a method based on information-theoretic view perspective using information gain as the segmentation criterion, which mainly considers the information that exists between the local nodes and the predicted output [11]. We filtered the segmentation criteria by GridSearchCV in two ways: 'gini', 'entropy'.

\section{Reference}
[1] Qi, Y., 2012. Random forest for bioinformatics. In Ensemble machine learning (pp. 307-323). Springer, Boston, MA.
[2] Belgiu, M. and Drăguţ, L., 2016. Random forest in remote sensing: A review of applications and future directions. ISPRS journal of photogrammetry and remote sensing, 114, pp.24-31.
[3] Svetnik, V., Liaw, A., Tong, C., Culberson, J.C., Sheridan, R.P. and Feuston, B.P., 2003. Random forest: a classification and regression tool for compound classification and QSAR modeling. Journal of chemical information and computer sciences, 43(6), pp.1947-1958.
[4] Strobl, C., Boulesteix, A.L., Zeileis, A. and Hothorn, T., 2007. Bias in random forest variable importance measures: Illustrations, sources and a solution. BMC bioinformatics, 8(1), pp.1-21.
[5] Paul, A., Mukherjee, D.P., Das, P., Gangopadhyay, A., Chintha, A.R. and Kundu, S., 2018. Improved random forest for classification. IEEE Transactions on Image Processing, 27(8), pp.4012-4024.
[6] Segal, M.R., 2004. Machine learning benchmarks and random forest regression.
[7] Probst, P. and Boulesteix, A.L., 2017. To tune or not to tune the number of trees in random forest. J. Mach. Learn. Res., 18(1), pp.6673-6690.
[8] Oshiro, T.M., Perez, P.S. and Baranauskas, J.A., 2012, July. How many trees in a random forest?. In International workshop on machine learning and data mining in pattern recognition (pp. 154-168). Springer, Berlin, Heidelberg.
[9] Timofeev, R., 2004. Classification and regression trees (CART) theory and applications. Humboldt University, Berlin, 54.
[10] Quinlan, J.R., 1986. Induction of decision trees. Machine learning, 1(1), pp.81-106.
[11] Yang, B.B., Gao, W. and Li, M., 2019, November. On the robust splitting criterion of random forest. In 2019 IEEE International Conference on Data Mining (ICDM) (pp. 1420-1425). IEEE.