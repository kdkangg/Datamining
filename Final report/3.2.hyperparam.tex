\section{\textbf{3.Method}}
In order to find the optimal result for the dataset, we tried to build a highly accurate model for data analysis. We have tried more than six machine learning algorithms and in particular it is important to adjust the parameters of each algorithm. Optimising the parameters by manual selection would be time consuming and inefficient, especially if there were too many parameters [1][2]. We use the Grid Search, which is a classical hyperparametric optimisation method [3], with cross-validation. In general, the grid search is an exhaustive search based on a defined subset of the hyperparameter space. The hyperparameters are specified by the number of steps, the minimum value (lower bound), the maximum value (upper bound) and three different scales are available: logarithmic scale, linear scale and quadratic scale. Performance metrics exist to evaluate the performance of the different combinations [4]. However, when the data set is large, the processing time can be infinitely longer [5]. Hence, the ability to handle low-latitude data sets is superior to that of high-latitude data sets. But since the hyperparameters handled by the algorithm are usually independent of each other, grid search can be easily parallelized [3]. Cross-validation, applied in conjunction with the grid research, is a widely used strategy for model selection or estimator risk assessment and its one of the most popular methods of model and parameter selection in statistics and machine learning [6]. It is characterised by its simplicity and (apparent) generality and has many results in terms of the model selection performance of the cross-validation procedure [7].
Scikit-learn can evaluate the performance of the estimator and select parameters using cross-validation. Cross-validation assigns computations to several cores by wrapping the estimator in a GridSearchCV object，where the “CV” stands for “cross-validated” [8]. Most notably, GridSearchCV hardly falls into local optimisation and is compatible with almost every supervised learning algorithm, so we applied it to the six machine learning algorithms we chose. Using GridSearchCV, we found the optimal parameters for the voting algorithm [9].


\section{Reference}
[1] Friedrichs, F. and Igel, C., 2005. Evolutionary tuning of multiple SVM parameters. Neurocomputing, 64, pp.107-117.
[2] Rossi, A.L.D. and de Carvalho, A.C., 2008, October. Bio-inspired optimization techniques for svm parameter tuning. In 2008 10th Brazilian Symposium on Neural Networks (pp. 57-62). IEEE.
[3] Liashchynskyi, P. and Liashchynskyi, P., 2019. Grid search, random search, genetic algorithm: A big comparison for NAS. arXiv preprint arXiv:1912.06059.
[4] Syarif, I., Prugel-Bennett, A. and Wills, G., 2016. SVM parameter optimization using grid search and genetic algorithm to improve classification performance. Telkomnika, 14(4), p.1502.
[5] Huang, Q., Mao, J. and Liu, Y., 2012, November. An improved grid search algorithm of SVR parameters optimization. In 2012 IEEE 14th International Conference on Communication Technology (pp. 1022-1026). IEEE.
[6] Lei, J., 2020. Cross-validation with confidence. Journal of the American Statistical Association, 115(532), pp.1978-1997.
[7] Arlot, S. and Celisse, A., 2010. A survey of cross-validation procedures for model selection. Statistics surveys, 4, pp.40-79.
[8] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V. and Vanderplas, J., 2011. Scikit-learn: Machine learning in Python. the Journal of machine Learning research, 12, pp.2825-2830.
[9] Zhao, S., Mao, X., Lin, H., Yin, H. and Xu, P., 2020. Machine Learning Prediction for 50 Anti-Cancer Food Molecules from 968 Anti-Cancer Drugs.



